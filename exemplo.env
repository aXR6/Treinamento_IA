# =====================================================================
# .env example
# =====================================================================
# — PostgreSQL Connection
PG_HOST=172.16.187.133
PG_PORT=5432
PG_USER=vector_store
PG_PASSWORD=senha
PG_DB_PDF=vector_store_pdf
PG_DB_QA=vector_store_pdf_qs
PG_DB_CVE=cve

# — Modelos de Embedding & Chunking
OLLAMA_EMBEDDING_MODEL=mixedbread-ai/mxbai-embed-large-v1
SERAFIM_EMBEDDING_MODEL=PORTULAN/serafim-900m-portuguese-pt-sentence-encoder-ir
MINILM_L6_V2=sentence-transformers/all-MiniLM-L6-v2
MINILM_L12_V2=sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2
MPNET_EMBEDDING_MODEL=sentence-transformers/all-mpnet-base-v2
QWEN3_EMBEDDING_MODEL=Qwen/Qwen3-Embedding-8B
# Modelo SBERT (unificado para chunking e embedding)
SBERT_MODEL_NAME=${OLLAMA_EMBEDDING_MODEL}

# Modelo da Hugging Face para treinamento (opcional)
TRAINING_MODEL_NAME=deepseek-ai/DeepSeek-R1-Distill-Llama-8B
# Avaliação a cada N passos
EVAL_STEPS=500
# Porcentagem do dataset usada para validação
VALIDATION_SPLIT=0.1
# Hiperpar\u00e2metros do treinamento
LEARNING_RATE=5e-5
WEIGHT_DECAY=0.01
WARMUP_STEPS=100
GRADIENT_ACCUMULATION_STEPS=1
LR_SCHEDULER_TYPE=linear
# TOKENIZE_NUM_PROC=12      # CPUs com 12 núcleos
# DATALOADER_NUM_WORKERS=12 # idem acima
# Modelos para geração de perguntas e respostas
##PT-BR
QG_MODEL=Narrativa/mT5-base-finetuned-tydiQA-question-generation
QA_MODEL=Narrativa/mT5-base-finetuned-tydiQA-xqa
##EN
#QG_MODEL=valhalla/t5-base-qa-qg-hl
#QA_MODEL=${QG_MODEL}

# — Embeddings dimensions
DIM_MXBAI=1024
DIM_SERAFIM=1536
DIM_MINILM_L6=384
DIM_MINIL12=384
DIM_MPNET=768
DIM_QWEN3=4096

# — Parâmetros OCR
OCR_THRESHOLD=100
OCR_LANGUAGES=eng+por
TESSERACT_CONFIG=""
PDF2IMAGE_TIMEOUT=600

# — Parâmetros de chunking (recomende <=512 para perguntas/respostas)
CHUNK_SIZE=512
CHUNK_OVERLAP=128
SLIDING_WINDOW_OVERLAP_RATIO=0.25
MAX_SEQ_LENGTH=128
SEPARATORS="\n\n|\n|.|!|?|;"

