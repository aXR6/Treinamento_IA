# üß† Extra√ß√£o, Chunking e Indexa√ß√£o Inteligente de Documentos PDF/DOCX

## Vis√£o Geral

Este projeto oferece um pipeline completo para extra√ß√£o, processamento, chunking, enriquecimento sem√¢ntico e indexa√ß√£o de documentos PDF e DOCX em bancos de dados MongoDB e PostgreSQL. Permite buscas sem√¢nticas com embeddings, busca h√≠brida (RAG), re-ranking e monitoramento de m√©tricas.

O sistema integra OCR, NLP, sumariza√ß√£o, NER, expans√£o de queries e m√∫ltiplas estrat√©gias de extra√ß√£o, possibilitando consultas avan√ßadas e r√°pidas sobre grandes volumes de documentos t√©cnicos ou acad√™micos.

---

## Funcionalidades

- **Extra√ß√£o avan√ßada de texto** de PDFs/DOCX usando OCR, Tika, PyPDF, Unstructured, PDFMiner, entre outros.
- **Chunking inteligente:** divis√£o dos documentos em se√ß√µes/chunks, com regras hier√°rquicas, sliding window e enriquecimento sem√¢ntico (sumariza√ß√£o, NER, par√°frase).
- **Expans√£o de queries:** aplica√ß√£o de sin√¥nimos e termos relacionados via WordNet para melhorar o recall.
- **Gera√ß√£o autom√°tica de embeddings** (Ollama ou SBERT local) para cada chunk, permitindo busca vetorial.
- **Re-ranking com Cross-Encoder:** p√≥s-processamento dos resultados de busca vetorial para maior precis√£o.
- **Monitoramento de m√©tricas:** coleta de contagem de queries, lat√™ncia e n√∫mero de resultados via Prometheus (`/metrics`).
- **Indexa√ß√£o de dados:**
    - **MongoDB:** armazenamento de chunks, metadados e arquivos bin√°rios.
    - **PostgreSQL:** armazenamento de chunks, metadados, embeddings (pgvector), `tsv_full` para busca h√≠brida e fun√ß√£o `match_documents_hybrid`.
- **Triggers e √≠ndices autom√°ticos:** mant√©m `tsv_full` sincronizado e utiliza √≠ndices vetoriais (HNSW/IVFFlat) e trigramas para metadata.
- **Interface CLI intuitiva:** menu para sele√ß√£o de estrat√©gia de extra√ß√£o, banco de dados, chunking e embeddings.
- **Processamento em lote** de pastas com barra de progresso.
- **Configura√ß√£o flex√≠vel** via `.env`.
- **Compat√≠vel com ETL, API REST e orquestradores** como n8n.

---

## Arquitetura dos C√≥digos

<details>
<summary><strong>main.py</strong></summary>
Gerencia o fluxo principal do CLI: sele√ß√£o de estrat√©gias, banco de dados, processamento e coordena√ß√£o dos m√≥dulos.
</details>

<details>
<summary><strong>config.py</strong></summary>
Carrega configura√ß√µes do projeto a partir do `.env` e define vari√°veis globais.
</details>

<details>
<summary><strong>extractors.py</strong></summary>
Implementa estrat√©gias de extra√ß√£o de texto (PyPDF, PDFMiner, Unstructured, OCR, etc.) e extra√ß√£o de metadados.
</details>

<details>
<summary><strong>adaptive_chunker.py</strong></summary>
Divide o texto extra√≠do em chunks com chunking hier√°rquico, sliding window, sumariza√ß√£o, NER, par√°frase e query expansion.
</details>

<details>
<summary><strong>metrics.py</strong></summary>
Coleta m√©tricas de RAG (contagem de queries, lat√™ncia, n√∫mero de resultados) e exp√µe endpoint Prometheus.
</details>

<details>
<summary><strong>pg_storage.py</strong></summary>
Integra√ß√£o com PostgreSQL: gera√ß√£o e inser√ß√£o de embeddings, chunking sem√¢ntico, re-ranking com cross-encoder e registro de m√©tricas.
</details>

<details>
<summary><strong>storage.py</strong></summary>
Persist√™ncia no MongoDB, salvando metadados, arquivos bin√°rios e integra√ß√£o com GridFS.
</details>

<details>
<summary><strong>utils.py</strong></summary>
Fun√ß√µes auxiliares para logging, valida√ß√£o de arquivos, gera√ß√£o de relat√≥rios e filtragem de par√°grafos.
</details>

---

## Estrutura do Banco de Dados (PostgreSQL)

- **Tabela `documents`:** cada chunk √© uma linha com `content`, `metadata` (`JSONB`), `embedding` (`VECTOR`) e `tsv_full`.
- **Triggers:** mant√™m `tsv_full` atualizado automaticamente.
- **√çndices:**
    - **Vetoriais:** HNSW e IVFFlat via pgvector.
    - **Full-text:** GIN em `tsv_full`.
    - **Trigramas:** GiST+pg_trgm em campos cr√≠ticos de metadata.
- **Fun√ß√µes:** `match_documents_hybrid` e `match_documents_precise` para busca h√≠brida.

---

## Como Usar

### Pr√©-requisitos

- Python 3.8+
- PostgreSQL 15+ com extens√£o `pgvector`
- MongoDB (opcional)
- Depend√™ncias Python (`requirements.txt`)
- API de embeddings (Ollama ou SBERT local)

### Configura√ß√£o

1. Ajuste o `.env` com suas credenciais e par√¢metros.
2. Aplique as DDLs no PostgreSQL.
3. Instale depend√™ncias:

    ```sh
    pip install -r requirements.txt
    ```

### Configura√ß√£o do NLTK

#### 1. Instalar corpora via APT

Em Debian 12, existem pacotes que fornecem tanto o c√≥digo do NLTK quanto muitos dos datasets mais usados.

```bash
sudo apt-get update                                 # Atualiza lista de pacotes  
sudo apt-get install -y python3-nltk nltk-data      # Instala o NLTK e os dados via reposit√≥rio oficial  
```
O pacote `python3-nltk` traz a biblioteca em si e algumas depend√™ncias.  
O pacote `nltk-data` inclui boa parte dos corpora ‚Äúpopular‚Äù j√° prontos, evitando downloads via HTTP direto dos servidores do NLTK.

#### 2. Download manual para um diret√≥rio local

Se precisar apenas de alguns corpora espec√≠ficos, voc√™ pode baix√°-los manualmente e armazenar numa pasta local:

Crie o diret√≥rio de dados, por exemplo:

```bash
sudo mkdir -p /usr/local/share/nltk_data/{corpora,tokenizers,taggers}
sudo chown -R $USER: /usr/local/share/nltk_data
```

Baixe pacotes individuais do GitHub do projeto nltk_data. Exemplo:

```bash
cd /usr/local/share/nltk_data/corpora
wget https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/packages/corpora/stopwords.zip
unzip stopwords.zip
rm stopwords.zip
```

Aponte o NLTK para esse diret√≥rio (opcional, se n√£o estiver no padr√£o):

```python
import nltk
nltk.data.path.append('/usr/local/share/nltk_data')
```

### Execu√ß√£o

1. Inicie o servi√ßo de m√©tricas (Prometheus) se desejar:

    ```sh
    # j√° √© iniciado automaticamente pelo metrics.py na porta 8000
    ```

2. Rode o menu principal:

    ```sh
    python3 main.py
    ```

3. Selecione a estrat√©gia de extra√ß√£o, banco de dados e informe o arquivo ou pasta.

4. Acompanhe logs, progresso e consulte m√©tricas em [http://localhost:8000/metrics](http://localhost:8000/metrics).

---

## FAQ & Dicas

- Ajuste `min_cos_sim` e `min_score` em suas queries SQL para controlar precis√£o.
- Para alta precis√£o, utilize re-ranking e thresholds mais altos.
- Monitore Precision@k e Recall@k via dashboards Prometheus/Grafana.

---

## Commit Breve

**Autor:** Thalles Canela <<thalles@example.com>>  
**Data:** 2025-05-20 12:34:56 -0300

### feat(rag): expans√£o de queries, re-ranking com cross-encoder e m√©tricas Prometheus

- **adaptive_chunker.py**
    - Implementada a fun√ß√£o `expand_query()` usando WordNet para expans√£o de queries (pseudo-relevance feedback).
    - Integrada a expans√£o de queries (`__query_expanded`) ao m√©todo `hierarchical_chunk()` para enriquecer as buscas.
    - Mantida a l√≥gica existente de chunking e enriquecimento (sumariza√ß√£o, NER, par√°frase).
    - Adicionado helper `get_cross_encoder()` para carregar CrossEncoder do Sentence-Transformers.

- **pg_storage.py**
    - `generate_embedding()` mantido inalterado.
    - Adicionada fun√ß√£o `rerank_with_cross_encoder()` para re-ranking dos resultados RAG via cross-encoder.
    - Fun√ß√£o `save_to_postgres()` agora decorada com `@record_metrics` (de metrics.py).
    - `save_to_postgres()`:
        - Insere semantic chunks normalmente.
        - Aplica re-ranking com cross-encoder nos chunks inseridos.
        - Retorna documentos reranqueados.

- **metrics.py** (novo)
    - Integra√ß√£o com Prometheus:
        - M√©tricas: `QUERY_COUNT`, `QUERY_LATENCY`, `RESULT_COUNT`.
        - Decorator `record_metrics` para medi√ß√£o autom√°tica.
        - Servidor HTTP na porta 8000 expondo `/metrics`.

- **README.md**
    - Documenta√ß√£o atualizada para:
        - Expans√£o de queries.
        - Passo de re-ranking com cross-encoder.
        - Monitoramento de m√©tricas com Prometheus.
        - Instru√ß√µes de uso do endpoint `/metrics`.

- **requirements.txt**
    - Adicionada depend√™ncia `prometheus_client`.

Essas mudan√ßas aprimoram a precis√£o do RAG com pseudo-relevance feedback, re-ranking detalhado e monitoramento em tempo real das m√©tricas de performance das queries.

## Autoria

Desenvolvido por Thalles Canela para workflows de IA e pesquisa documental em larga escala.
