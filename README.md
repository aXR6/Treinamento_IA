# üß† Treinamento e Indexa√ß√£o de Documentos

> **Este reposit√≥rio √© um _fork_ de [aXR6/Extrair_PDF_Agent_IA](https://github.com/aXR6/Extrair_PDF_Agent_IA).**  
> Mant√©m as capacidades de extra√ß√£o e busca em PDFs/DOCX, com foco principal no **treinamento de modelos da Hugging Face** usando textos armazenados no PostgreSQL.

---

## üöÄ Come√ßando

### Instala√ß√£o

Certifique-se de ter o Python 3.9 ou superior instalado.

Clone o reposit√≥rio e utilize o script `init-env.sh` para criar e ativar o
ambiente virtual automaticamente:

```bash
git clone https://github.com/aXR6/Treinamento_IA
cd Treinamento_IA
source init-env.sh
```

O script cria o diret√≥rio `.venv`, instala todas as depend√™ncias de
`requirements.txt` e deixa o ambiente pronto para uso.

Configure o arquivo `.env` (veja `exemplo.env`) e execute:

```bash
python3 main.py
```

### Vari√°veis de Ambiente

O arquivo `.env` possibilita ajustar diversos par√¢metros do projeto:

- **Modelos de embedding**: `OLLAMA_EMBEDDING_MODEL`, `SERAFIM_EMBEDDING_MODEL`,
  `MINILM_L6_V2`, `MINILM_L12_V2` e `MPNET_EMBEDDING_MODEL`. Defina
  `SBERT_MODEL_NAME` para escolher qual ser√° o padr√£o.
- **Dimens√µes**: `DIM_MXBAI`, `DIM_SERAFIM`, `DIM_MINILM_L6`, `DIM_MINIL12` e
  `DIM_MPNET` indicam o tamanho dos vetores em `documents_<dim>`.
- **OCR**: `OCR_LANGUAGES`, `TESSERACT_CONFIG`, `OCR_THRESHOLD` e
  `PDF2IMAGE_TIMEOUT` controlam a extra√ß√£o de texto via OCR.
- **Chunking**: `CHUNK_SIZE`, `CHUNK_OVERLAP`, `SLIDING_WINDOW_OVERLAP_RATIO`,
  `MAX_SEQ_LENGTH` e `SEPARATORS` determinam como os textos s√£o divididos antes
  da gera√ß√£o dos embeddings. Caso `MAX_SEQ_LENGTH` n√£o seja definido, o padr√£o
  √© `128`.
- **Treinamento**: `TRAINING_MODEL_NAME`, `EVAL_STEPS` e `VALIDATION_SPLIT`
  personalizam o fine-tuning de modelos da Hugging Face.
- **Perguntas e Respostas**: `QG_MODEL` e `QA_MODEL` definem os modelos
  usados para gerar perguntas e respostas (padr√£o `valhalla/t5-base-qa-qg-hl`).
  Para textos em portugu√™s, experimente
  `pierreguillou/t5-base-qa-qg-hl-portuguese-squad_v1`.
  **Recomenda-se que `CHUNK_SIZE` seja de no m√°ximo 512 ao gerar perguntas e respostas.**
  A fun√ß√£o `generate_qa` limita automaticamente `doc_stride` para nunca exceder o
  tamanho m√°ximo suportado pelo tokenizer. Use `QA_EXPLICIT_PROMPT` para gerar
  respostas com `model.generate` a partir de um prompt "question: ... context:
  ...". Modelos generativos como `Narrativa/mT5-base-finetuned-tydiQA-xqa` devem
  ser carregados com `AutoModelForCausalLM` para que `generate` funcione. Exemplo:

  ```python
  from transformers import AutoTokenizer, AutoModelForCausalLM
  tok = AutoTokenizer.from_pretrained("Narrativa/mT5-base-finetuned-tydiQA-xqa")
  model = AutoModelForCausalLM.from_pretrained(
      "Narrativa/mT5-base-finetuned-tydiQA-xqa")
  prompt = f"question: {question_text} context: {context_text}"
  answer = tok.decode(model.generate(**tok(prompt, return_tensors='pt'))[0])
  ```
- **Outros**: `CSV_FULL` e `CSV_INCR` podem apontar para arquivos CSV locais de
  vulnerabilidades (opcional).

#### TyDiQA

Defina `QG_MODEL` como
`Narrativa/mT5-base-finetuned-tydiQA-question-generation` e `QA_MODEL` como
`Narrativa/mT5-base-finetuned-tydiQA-xqa`. Para obter os melhores resultados,
esse gerador de perguntas espera um prompt no formato
`answer: <resposta> context: <contexto>`.
A cobertura inclui os idiomas:
(Afrikaans, Albanian, Amharic, Arabic, Armenian, Azerbaijani, Basque, Belarusian, Bengali, Bulgarian, Burmese, Catalan, Cebuano, Chichewa, Chinese, Corsican, Czech, Danish, Dutch, English, Esperanto, Estonian, Filipino, Finnish, French, Galician, Georgian, German, Greek, Gujarati, Haitian Creole, Hausa, Hawaiian, Hebrew, Hindi, Hmong, Hungarian, Icelandic, Igbo, Indonesian, Irish, Italian, Japanese, Javanese, Kannada, Kazakh, Khmer, Korean, Kurdish, Kyrgyz, Lao, Latin, Latvian, Lithuanian, Luxembourgish, Macedonian, Malagasy, Malay, Malayalam, Maltese, Maori, Marathi, Mongolian, Nepali, Norwegian, Pashto, Persian, Polish, Portuguese, Punjabi, Romanian, Russian, Samoan, Scottish Gaelic, Serbian, Shona, Sindhi, Sinhala, Slovak, Slovenian, Somali, Sotho, Spanish, Sundanese, Swahili, Swedish, Tajik, Tamil, Telugu, Thai, Turkish, Ukrainian, Urdu, Uzbek, Vietnamese, Welsh, West Frisian, Xhosa, Yiddish, Yoruba, Zulu.), incluindo suporte para portugu√™s.

```python
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
tok = AutoTokenizer.from_pretrained("Narrativa/mT5-base-finetuned-tydiQA-question-generation")
model = AutoModelForSeq2SeqLM.from_pretrained("Narrativa/mT5-base-finetuned-tydiQA-question-generation")
prompt = f"answer: {answer_text} context: {context_text}"
question = tok.decode(model.generate(**tok(prompt, return_tensors='pt'))[0])
```

O prompt precisa dos prefixos `answer:` e `context:`.

#### Exemplo simplificado de `.env`

```env
PG_HOST=192.168.3.32
PG_PORT=5432
PG_USER=vector_store
PG_PASSWORD=senha
PG_DB_PDF=vector_store_pdf
PG_DB_QA=vector_store_pdf_qs

TRAINING_MODEL_NAME=deepseek-ai/DeepSeek-R1-Distill-Llama-8B
EVAL_STEPS=500
VALIDATION_SPLIT=0.1
QG_MODEL=valhalla/t5-base-qa-qg-hl
QA_MODEL=${QG_MODEL}
# Ativa prompt explicito no QA (opcional)
# QA_EXPLICIT_PROMPT=1
# QG_MODEL=Narrativa/mT5-base-finetuned-tydiQA-question-generation
# QA_MODEL=Narrativa/mT5-base-finetuned-tydiQA-xqa
```

### Estrutura do Projeto

```
‚îú‚îÄ main.py           # CLI para indexa√ß√£o e treinamento
‚îú‚îÄ training.py       # Fun√ß√µes de fine-tuning de modelos
‚îú‚îÄ init-env.sh       # Cria√ß√£o/ativa√ß√£o do virtualenv
‚îú‚îÄ BD_PostgreSQL/    # Scripts SQL da estrutura do banco
‚îú‚îÄ Srv/              # Microservi√ßo FastAPI para embeddings
```

---

## üèãÔ∏è Treinamento de Modelos

1. Defina o modelo desejado em `TRAINING_MODEL_NAME` no `.env`. Se n√£o definido, ser√° usado `deepseek-ai/DeepSeek-R1-Distill-Llama-8B`.
2. Processe seus documentos normalmente (op√ß√µes 1 a 7 do menu) para popular a tabela `public.documents_<dim>`.
3. Escolha a dimens√£o (op√ß√£o 3) e o dispositivo (op√ß√£o 4).
4. Acesse **8 - Treinamento** ou **9 - Treinamento QA**. Nos submenus voc√™ pode:
   - Definir a tabela de origem (op√ß√£o 3).
   - Ajustar √©pocas, batch size, passos de avalia√ß√£o e porcentagem de valida√ß√£o.
   - Ativar ou n√£o a detec√ß√£o autom√°tica de GPU.
   - Por fim, escolha **1 - Treinar modelo**.
5. O resultado √© salvo em uma pasta `MODELNAME_finetuned_<dim>` e o melhor modelo em `best_model`.

### Depend√™ncias

- `transformers`
- `sentence-transformers`
- `datasets`
- `accelerate`
- `torch`
- `psycopg2-binary`
- `question-generation`

`pg_storage.py` tenta importar o pacote `question_generation`. Caso a
depend√™ncia n√£o esteja dispon√≠vel, uma mensagem de erro √© registrada e a fun√ß√£o
`generate_qa` retorna pares vazios.

Todas as depend√™ncias est√£o listadas em `requirements.txt`.

---

## ‚öôÔ∏è Funcionalidades

- **Extra√ß√£o de Texto:** PDF, DOCX e imagens com m√∫ltiplas estrat√©gias
  (PyPDFLoader, PDFMiner, Unstructured, OCR, etc.) e reparo autom√°tico de PDFs.
- **Chunking Inteligente:** algoritmo hier√°rquico que agrupa par√°grafos,
  expande queries com WordNet e usa sliding window quando necess√°rio.
- **Embeddings e Indexa√ß√£o:** gera√ß√£o de vetores com SBERT e inser√ß√£o em
  streaming no PostgreSQL (pgvector), permitindo busca h√≠brida (RAG).
- **Pares de Pergunta/Resposta:** as tabelas `documents_<dim>` agora incluem as
  colunas `question` e `answer` para armazenar contextos j√° respondidos e
  otimizar workflows de RAG. Se a gera√ß√£o falhar, um *warning* indica o √≠ndice do
  chunk e o arquivo correspondente.
- **Re-ranking e M√©tricas:** Cross-Encoder para ordenar resultados e servidor
  Prometheus embutido para monitorar consultas.
- **CLI Interativo:** escolha de estrat√©gia, modelo, dimens√£o, dispositivo e
  par√¢metros de treinamento, com barra de progresso e logs detalhados.

---

## üìÑ Licen√ßa

Consulte o arquivo [LICENSE](./LICENSE) para mais informa√ß√µes.

---
